[TOC]

##### 1.海量数据排序问题

题目：如果有1TB的数据需要排序，但是只有32G的内存，如何排序处理？

>思路：使用**外排序**，传统的内排序算法，即数据一次全部载入内存中的做法显然是不行的，海量数据不可能一次全部载入内存。外排序采用分块的方法，首先将数据分块，对块内的数据选择一种高效的内排序策略进行排序，然后采用归并排序的思想对所有的块进行排序，得到所有数据的一个有序序列。
>
>首先明白内存中输入缓冲区、输出缓冲区的概念，还要留些操作需要的内存。
>
>1. 把磁盘上的1TB数据分割为40块，每份25GB；
>2. 顺序地将每份25GB数据读入内存，使用快速排序进行内部排序；
>3. 把排序好的数据（25GB）存放回磁盘；
>4. 循环40次，所有的40个块都已各自排序了；
>5. 从40个块中分别读取25G/40=0.625G进入内存（40个输入缓冲区）；
>6. 执行40路合并，将合并结果存储于2GB基于内存的输入缓冲区中，当输出缓存区写满了2G时，写入硬盘上最终文件，并且清空输出缓冲区。同时，当40个输入缓冲区中任何一个处理完毕时，写入该输入缓冲区所对应的块中的写一个0.625G，直到全部处理完成。
>
>磁盘的I/O通常是越少越好，可以将第5,6步改为先做8路归并排序，把8个块归并为1路，然后在做5路到1路的合并操作。

##### 2.海量数据中位数问题

题目：有一个文件有10G个**整数**，乱序排列，要求找出中位数。内存限制为2G。

>思路：将整型的每1字节作为一个关键字，一个整型可以拆为4个keys，而且最高位的key越大，整数就越大。如果高位的key相同，则比较次高位的key，整个过程类似于字符串的字典序。
>
>1. 将10G的整数每2G读入一次内存，然后遍历这些数据，每个数据用位运算">>"取出最高的8位，这8bits最多表示255个桶，那么可以根据8bits的值来确定丢入第几个桶，最后把每个桶写入一个磁盘文件，同时内存中统计每个桶内数据的数量，这需要255个整型空间即可。
>2. 根据内存中255个桶内的数量，计算中位数在第几个桶中。平均而言，每个桶的数据大小应该在10G/128=80M左右，这时可以直接将中位数所在的桶进行排序，得出答案。如果这个桶的数据大于2G，那么像第1步中那样，按照整数的次高8位进行桶排序，直至中位数所在的桶的可以读入内存进行快速排序。
>
>整个过程的时间复杂度在$O(n)$ 线性级别，主要的时间消耗在第一步的数据转移到磁盘上，第二步将桶的数据读入内存排序其实很快。注意这里不能使用哈希表，因为哈希映射之后是无序的，哈希比较适合统计。

##### 3.海量数据的Top K问题

题目：有一个1G大小的文件，里面每一行是一个次，词的大小不超过16字节，内存限制为1M，返回频数最高的100个词。

>思路：顺序读取文件，对于每个词，取hash(x)%5000（用哈希映射）,然后按照该值存到5000个小文件中，每个文件的大小大概是200K（包含12500词）。如果其中有的文件大小超过了1M，那么按照类似地方法继续使用哈希划分。
>
>1. 对于每个小文件，统计**每个文件中出现的词及相应的频率**，并取出出现频率最大的100个词（这个操作可以用大小为堆完成），并把100词及相应的频率转存入文件，这样又得到了5000个文件（5000个文件，每个100词，大小约为8M，内存放不下，只能再次转存）。
>2. 将5000个文件进行归并排序，即多路归并排序。
>
>这里注意，无序数组的统计，优先使用哈希表，时间复杂度为$O(n)​$，而其它的方法，必须先排序再统计，时间复杂度直接高了一个量级。另外，我们这里的哈希函数的输出都要对数组长度取余，这个数组长度的大小决定了哈希的划分，这里为5000，就只能划分为5000个单元。

##### 4.重复数据的频度排序问题

题目：

>1. 顺序读取10个文件，按照hash(query)%10的结果将query写入到另外10个文件中，这样新生成的文件每个大小也大概是1G（但是此时一个query只会出现在一个文件中）。
>2. 找一台内存2G左右的机器，依次对每个文件使用hash_map(query,query_count)来统计每个query出现的次数，利用然后按照query_count进行排序，将排好序的 (query,query_count)输出到文件中。这样得到了10个排序好的文件。
>3. 对这10个文件进行归并排序（内排序和外排序相结合）。

##### 5.两个文件是否有重复的词条url

题目：给定a,b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，找出a,b文件中共同的url。

>思路，使用哈希表将文件分成1000个小文件，a,b按照相同的哈希函数分，那么只需要比较对应的a_1,b_1...a_n,b_n对是否有相同的url就可以了。操作如下：
>
>1. 估计每个文件的大小为5Gx64=320G，内存为4G，那么分为1000个小文件。遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,...a999)，每个文件大小约是300M。
>2. 遍历文件b，采取和a相同的方式将url 分别存储到 1000个小文件中，这样处理后，所有可能相同的url都在对应的小文件中，不对应的小文件不可能有相同的url。
>3. 求每对小文件中相同的url时，把其中一个文件的url存储到hash_set中，然后遍历另一个文件中的每个url，检查其是否在刚才构建的hash_set中，如果是，那就有共同的url，存到文件就可以了。
>
>==这里的hash用到了两次：第一次使用hash函数对url进行“分桶”，第二次用来去重、查找。==