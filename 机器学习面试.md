[TOC]

#### 训练与测试

##### 1.什么是过拟合，如何解决？

> 随着训练的进行，模型复杂度增加，在训练集上的误差逐渐减小，但是在验证集上的误差逐渐增大，模型泛化性能不好。
>
> 降低过拟合的方法如下：
>
> 1. **正则化**：
> 2. L1正则化：目标函数中增加所有权重参数的绝对值，逼迫所有的权值尽可能为零，另外权值稀疏化可以实现特征的自动选择。
> 3. L2正则化：目标函数中增加权重参数的平方之和，因为过拟合的时候模型会顾及每一个点，最终形成的拟合函数波动很大，在某些很小的区间里，函数值的变化非常剧烈，也就是某些权值非常大。L2正则化的加入就惩罚了权重变大的趋势。
> 4. **随机失活**：训练的时候，让神经元以超参数p的概率被随机失活，因此每个权重随机参与，使得任意的权值都不是不可或缺的，效果类似于数量巨大的模型集成。
> 5. **Batch-Normalization**：相当于做一个线性变换，每次SGD的时候通过mini-batch来对相应的激活值做规范化操作，使输出值各个维度的均值为0，方差为1.
> 6. **提前终止**：使用交叉验证法检测每次训练完后的错误率，取错误率最小的模型。

##### 2.什么是批量权值归一化，有什么作用？

> 相当于做一个线性变换，每次SGD的时候通过mini-batch来对相应的激活值做规范化操作，使输出值各个维度的均值为0，方差为1。最后的"scale and shift"操作则是为了让因训练而“刻意”加上的BN能够有可能还原最初的输入，这样可以提高模型的容纳能力。
>
> 具体操作时， BN利用隐藏层输出结果的均值和方差来标准化每一层特征的分布，并且维护所有的mini-batch数据的均值与方差，最后利用样本的均值与方差的无偏估计量用于测试时使用。
>
> BN应作用在非线性映射前，即对$x=Wu+b$做规范化，也就是激活函数前。
>
> BN层的作用有两个：
>
> 1. 神经网络各层的输出，经过层内操作的作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增加而增大，这个问题称为covariate shift。但是，仅仅将输入均值为0，方差为1并不能保证分布相同，这个问题并没有解决。这里的效果就是使模型的训练更加稳定、更快。
> 2. 说到底还是为了防止“梯度弥散”（梯度爆炸和消失），通过将激活值规范为均值和方差一致的手段使得原本会减小的activation的scale变大，同样也会使增大的变小，因为BN层抹去了权值逐层的scale的影响。
>
> 在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。

#### 数据预处理

##### 1.常用的数据预处理操作

> 预处理流程大概如下：
>
> 1. 去除唯一属性；
> 2. 处理缺失值：删除或者补全。
> 3. 如果样本属性的距离可以度量，那么使用均值插补。如果不可度量，那么使用众数插补。
> 4. 同类均值插补，即先将样本进行分类，然后以该类中样本的均值来插补缺失值。
> 5. 建模预测，将缺失的属性作为预测目标来预测，将数据集按照是否含有特定属性的缺失值分为两类，利用现有的机器学习算法对待预测数据集的缺失值进行预测。当该属性值与其他属性无关时，那么预测的结果毫无意义，但是如果预测的结果相当准确，那么说明这个缺失属性和其他属性的相关性特别强，没有必要纳入数据集。
> 6. 高维映射，将属性映射到高维空间，做one-hot处理，将K个离散值的属性值扩展为K+1个属性值，若该属性值缺失，则扩展后的第K+1个属性值置为1。虽然很精确，但是计算量很大，只有在样本量很大时效果才会比较好。
> 7. 特征编码：
> 8. 特征二元化：即设定阈值，将数值型的属性转换为布尔值的属性。
> 9. 独热编码（one-hot）：能够处理非数值属性，在一定程度上扩充了特征，但是编码后的属性是稀缺的，存在大量的零分量。
> 10. 数据标准化：
> 11. 数据标准化：即将样本的属性缩放到某个指定的范围。原因：1.消除样本不同属性具有不同量级时的影响，数量级的差异将导致量级较大的属性占据主导地位；2.数量级的差异将导致收敛的速度变慢，梯度下降走之字形；3.依赖于样本距离的算法对于数据的数量级非常敏感。
> 12. 标准化方式有：min-max标准化，z-score标准化（基于原始数据的均值和标准差进行数据的标准化，均值和方差都是在样本集上定义的，而不是在单个样本上定义的，标准化是针对某个属性的，需要用到所有样本在该属性上的值）。
> 13. 正则化：数据正则化是将样本的某个范数缩放到1，正则化的过程是针对单个样本的。
> 14. 特征选择：从给定特征集上选出相关子特征集的过程称为特征选择，主要原因为：减轻维数灾难；降低学习任务的难度。
> 15. 过滤式选择：先对数据集进行特征选择，然后再训练分类器，特征选择与后续的学习器无关；
> 16. 包裹式选择：直接把最终要使用的学习器的性能作为特征子集的评价原则，优点是针对特定学习器进行优化，效果因而较好，缺点是由于特征选择过程中需要多次训练学习器，计算开销较大。

##### 2.类别不平衡问题如何解决？

>首先要理解：对于平衡的数据，我们一般都用准确率，作为评估的标准，这种标准的默认假设前提是“数据是平衡的，正反例的重要性一样，二分类分类器的阈值是0.5”。这种情况下，用准确率对分类器进行评估是合理的。而当类别不平衡时，准确率是很具有迷惑性的，而且意义不大。主要的评估方法如下：
>
>* ROC曲线的面积；
>* PR曲线的线下面积；
>* Precision@n，即将分类阈值设定得到恰好n个正例时分类器的 precision。
>
>类别不平衡问题如何解决？
>
>1. 直接对训练集的反类样例进行欠采样，去除一些反例使正反样例的数目相接近；
>2. 对训练集的正类样例进行“过采样”，即增加一些正例，使正反例的数目相接近；
>3. 直接基于原始训练集进行学习，但具体预测的时候，调整分类的阈值。
>
>注意“过采样”不能简单地直接对正例样本进行重复采样，否则会导致严重的过拟合，过采样的代表性算法是SMOTE，是通过对训练集的正例进行插值来产生额外的正例。

##### 3.缺失值如何处理，那些模型对缺失值敏感？

>首先介绍一些经验法则：
>
>1. 树模型对于缺失值的敏感度很低，大部分都可以在数据有缺失的时候使用；
>2. 涉及到距离度量时，如果计算两个点之间的距离，那么缺失数据就会变得比较重要，因为涉及到距离的问题，如果缺失值处理不当就会效果很差，比如K近邻算法和支持向量机。
>3. 线性模型的代价函数往往涉及到距离的度量计算，比如预测值和真实值之间的差别，这容易导致对于缺失值的敏感；
>4. 神级网络的鲁棒性很强，对于数据缺失不是很敏感，但是一般需要大量的数据；
>5. 贝叶斯模型对于缺失数据也比较稳定，数据量很小的时候首推贝叶斯模型。
>
>因此，对于有缺失值的数据经过缺失值处理之后：
>
>1. 数据量如果很小，那么使用朴素贝叶斯；
>2. 数据量始终或较大，那么使用树模型，优先使用XGBoost；
>3. 数据量较大，可以使用神经网络；
>4. 避免使用距离度量相关的模型，比如SVM和KNN。

#### 损失函数

##### 1.为什么选择交叉熵作神经网络的损失函数?

> 神经网络如果选择Sigmoid函数作为激活函数，那么网络开始训练时，由于参数的随机初始化，激活函数很容易饱和，此时使用梯度下降会变得非常缓慢，而交叉熵可以抵消激活函数导数饱和的影响，避免了某些情况下的梯度消失问题。
>
> 另外，当误差较大时，使用交叉熵损失函数的梯度也较大，从而可以实现误差较大的网络的学习速率也较快，加速收敛。

#### LR常见问题

逻辑斯特回归模型中，输出$y=1$的对数几率是输入x的线性函数，这就是LR线性的本质。按照李航的学习方法三要素：

**模型**很简单就是$P(y_i=1|x_i)=\frac{e^{wx+b}}{e^{wx+b}+1}$，模型的要点就是用线性模型来拟合对数几率；

**策略**有两个：最大似然函数和对数损失，两者推导出的结果是一样的；

**算法**：即优化$\sum^N_{i}[y_iw\cdot x_i-\log(1+\exp(w\cdot x_i))]$。

将逻辑斯特回归模型推广到多项分类问题上比较容易，策略和算法不变。

##### 1.LR回归为什么要对特征进行离散化？

> 在工业界，很少直接将连续值作为LR模型的特征输入，而是将特征离散化为一系列的0,1特征，优点如下：
>
> 1. 离散值的增加和减少比较容易，易于模型的快速迭代；
> 2. <u>稀疏向量内积乘法运算速度比较快，</u>计算结果更容易存储，容易扩展；
> 3. <u>离散后的特征对于异常数据具有鲁棒性</u>：比如将年龄特征按区间划分，当出现异常值很大的错误年龄时，不会给模型造成很大的困扰；
> 4. 逻辑回归属于广义线性模型，表达能力有限，单变量离散化为N个后，<u>每个变量有单独的权重，相当于为模型引入了非线性，能够提高模型的表达能力，加大拟合；</u>
> 5. 特征离散化后，模型会更加的稳定，不会因为微小的值改变就产生较大的结果差异；
> 6. 特征离散后，起到了简化逻辑回归模型的作用，降低了模型过拟合的风险。

##### 2.LR和SVM的联系和区别：

> 联系：
>
> 1. LR和SVM都可以处理分类问题，而且一般都用于处理线性二分类问题，改进后可处理多分类问题。
> 2. 两个方法都可以增加不同的正则化项，如L1，L2，两个算法在实验中结果相近。
>
> 区别：
>
> 1. 目标函数上，SVM采用的合页损失函数，LR采用的对数损失，这两个损失函数都是增加了分类影响较大点的权重，减小了分类影响较小点的权重。
> 2. 实现方法，SVM只考虑支持向量去学习分类器，而LR通过非线性映射来减小对分类影响较小点的权值。
> 3. LR是参数模型，SVM是非参数模型。
> 4. 逻辑回归相对来说模型简单，作大规模线性分类时比较方便。而SVM转化为对偶问题时，分类只需要计算与少数几个支持向量的距离即可。

#### SVM常见问题

**模型**很简单，对于标签为+1,-1的数据，找到一个超平面，超平面的一侧为正，另一侧为负，$f(x_i)=sgn(w\cdot x_i+b)$。

**策略**就是点到这个超平面的距离最大。

转换成对偶形式后，分类决策函数只依赖于输入x和训练样本的内积。

##### 1.SVM的原理是什么?

>SVM是一个二分类模型，基本特征是在特征空间中寻找间隔最大化的分离超平面，作为分类器。
>
>1. 当样本线性可分的时候，通过找到间隔最大的划分超平面，学习一个线性分类器；
>2. 当样本近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器；
>3. 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机；
>
>能够完成推导，对偶问题，KKT条件分析。

##### 2.SVM为什么采用间隔最大化?

>训练数据线性可分时，存在无穷个分离超平面将数据正确分开。SVM利用间隔最大化求得最优分离超平面的解是唯一的。此时的分隔超平面是最鲁棒的，对未知实例的泛化能力最强。

##### 3.为什么SVM要引入对偶问题?

>1. 对偶问题的求解相对于原问题往往更加容易，对于最优化问题，约束条件的存在虽然减小了搜寻范围，但是求解上更加的困难，通过求解对偶问题，我们可以把目标函数和约束条件融入拉格朗日函数，有可能会简化问题的求解。
>2. SVM原问题转换为对偶问题后可以引入核函数，将模型推广到非线性问题上。

##### 4.为什么要引入核函数？

>当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使样本在高维特征空间内线性可分。对偶问题求解时和模型预测时，可以只定义核函数，而不需要定义映射函数。一般来说，特征空间维数可能很高，甚至是无穷维的，因此直接计算映射后的内积比较困难，而特征空间的内积等于它们在原始样本空间通过核函数计算的结果，计算很方便。

##### 5.常用的核函数有哪些？

>$$
>K(x,x_i)=\exp(-\frac{||x-x_i||^2}{\sigma^2})
>$$
>
>高斯径向基核函数是局部性很强的核函数，将原始空间映射为无穷空间。不过，当$\sigma$选的很大的时候，高次特征的权重衰减的非常快，实际上相当于一个低维子空间；反过来，如果选的过小，则可以将数据任意线性可分，这并不是一件好事，因为随之而来的是很严重的过拟合问题。总的来说，通过调参$\sigma$，高斯核具有很大的灵活性，是使用最广泛的核函数。
>
>这里无穷维的理解，结合指数函数的泰勒展开式是无穷维多项式。

##### 6.为什么SVM对缺失数据敏感？

>待回答。
>
>

##### 7.核函数的应用场景

>一般选择线性核和高斯径向基核。
>
>* 线性核：主要用于线性可分的情况，参数少，速度快，对于一般的数据，足够了；
>* 高斯核：参数多，调参量大，如果特征的数量大，和样本数量差不多，这时候使用线性核；如果特征的数量少，样本数量一般，那么选用高斯核。（如果数据量很大，那么直接用神经网络）

##### 8.如何实现多分类

>OvO：一对一，针对任意两个类训练一个分类器，如果有k个类，总共需要训练$k(k-1)/2$个分类器，这样当有一个新样本要来的时候，用这$k(k-1)/2$个分类器进行测试，每当被判定某一类的时候，这类就加一，最后票数多的类别被认为该样本的类。
>
>OvR：一对多，就是对每个类都训练出一个分类器，由于SVM是二分类，所以将此分类器的两类设定为目标类的一类，其余类是另外一类。这样k个类可以训练出k个分类器，每当有一个新样本来的时候，用这k个分类器来测试，哪个分类器的概率高，那么这个样本就属于哪一类。但是这种方法不太好，产生的偏差比较好。

##### 9.SVM常见的库，有哪些参数需要调节？

>用sklearn实现的，采用sklearn.svm.SVC设置的参数，这个函数是基于libsvm实现的，对于二次规划问题的求解算法是SMO。SVC函数的训练时间随训练样本平方级增长，不适合训练超过10000的样本。对于多分类问题，SVC采用的是one-vs-one投票机制，需要两两类建立分类器，训练的时间比较长。
>
>`sklearn.svm.SVC(C=1.0,kernel='rbf',degree=3,gamma='auto',coef0=0.0,shrinking=True,probability=False,tol=0.001,cache_size=200,class_weight=None,verbost=False,max_iter=-1,decision_function_shape=None,random_state=None)`
>
>参数如下：
>
>* `C`：惩罚参数默认为1，C越大，相当于对松弛变量的惩罚越大，趋近于对训练集全分对的情况，容易造成过拟合，反之则容错性较高，泛化能力强。
>* `gamma`：即所谓的$\sigma$参数，默认是auto，即1/n_feature。
>* `degree`：多项式核函数的维度，默认是3，仅对多项式核有用。
>* `coef0`：核函数的常数项，相当于多项式的常数项，仅对sigmoid和多项式核有用。
>* `tol`：停止训练的误差大小，默认是1e-3。

##### 10.SVM如何输出概率值？

>SVM模型的输出为$sign(\sum y_i\alpha_ik(x_i,x)+b)$， 设$f=\sum y_i\alpha_i k(x_i,x)+b$，我们通过sigmoid-fitting的方式学习一个线性分类器，将标准SVM的输出结果进行后处理，转换为后验概率值$ P(y=1|f)=\frac{1}{1+\exp(Af+B)} $。
>
>使用$(f_i,t_i)$来拟合，其中$t_i=\frac{y_i+1}{2},y_i\in\{-1,1\}$，极小化训练集上的负对数似然函数
>
>$ \min \sum t_i\log(p_i) +(1-t_i)\log(1-p_i) ​$。
>
>一般情况下，会对标签值$t$做平滑处理，防止Sigmoid函数对于数轴两端的值变化不敏感带来的区分困难。

#### 聚类常见问题

##### 1.聚类的概念

>聚类就是按照**距离准则** 把一个数据集分割为不同的类或者簇，使得**同一个簇内的数据对象的相似性**尽可能大，同时不在同一个簇内的数据对象的差异性也尽可能大，即聚类后同一类的数据尽可能聚集到一起，不同的数据尽可能分离。
>
>聚类算法具体分为：基于划分的聚类算法（K-means，学习向量量化LVQ）、基于模型的聚类算法（高斯混合聚类）、基于密度的聚类算法（DBSCAN），基于层次的聚类算法。

##### 2.衡量聚类算法优劣的标准有哪些

>不同的聚类算法有不同的优劣和不同的适用条件，具体评价标准如下：
>
>1. 算法的预处理能力：处理大的数据的能力（即算法的复杂度）；处理噪声的能力；处理任意数据分布形状的能力。
>2. 算法是否需要预设条件：是否需要预先知道聚类个数，是否需要用户给出领域知识。
>3. 算法的数据输入属性：算法处理的结构与数据输入的顺序是否相关，也就是说算法是否独立于数据输入顺序；算法处理多属性数据的能力，算法对数据维数是否敏感，对数据的类型要求（数值型，无序属性）。

##### 3.k-means的流程和优缺点

>主要流程：
>
>1. 随机选择k个对象作为k个簇的中心；
>2. 对剩余的每个对象，根据它与各个簇中心的距离，将它赋给最近的簇；
>3. 重新计算每个簇的平均值，更新为新的簇中心；
>4. 不断重复2,3，直至准则函数收敛。
>
>优点：
>
>1. 对于大型数据集也是简单高效，时间复杂度和空间复杂度低；
>
>缺点：
>
>1. 主要缺点是数据量大时容易陷入局部最优，这是贪婪算法的通病；
>2. 需要预先设定K值，对最初K个点的选取非常敏感；
>3. 对噪声和异常值非常敏感；
>4. 只适用于数值型数据；
>5. 不适用于非凸形状分布的数据（不规则形状）。

##### 4.基于密度的聚类（DBSCAN）的优缺点

>优点：
>
>1. 对噪声不敏感；
>2. 适用于任意数据形状的聚类；
>
>缺点：
>
>1. 聚类的结果和参数（半径、领域最小点数量）选取有很大关系；
>2. DBSCAN使用固定的密度参数识别聚类，但是当聚类的稀疏程度不同时，相同的判别标准可能会破坏聚类的自然结构，即较稀疏的聚类被划分为多个类或者密度较大且离得较近的类被合并为一个聚类。

##### 5.DBSCAN聚类有哪些参数，如何选取?

>DBSCAN需要输入两个参数：半径，即核心对象的邻域大小；MinPts，领域内最小点个数。
>
>k-距离：给定数据集，对任意点计算它与数据集其它点的距离，k-距离是指这个点到其他点第k近的距离。
>
>如何确定半径：根据计算的所有点的k-距离，对其进行排序，找出k-距离变化曲线的急剧变化点，最为半径的取值。这里的k就是参数 MinPts，也就是说，这个方法半径的选取是基于MinPts和数据集点的分布的。

##### 6.模型聚类主要的方法有哪些，优缺点

>模型聚类主要是基于概率模型和神经网络模型的方法，这里的概率模型主要是概率生成模型，同一“类”的数据属于同一种概率分布，数据集由混合的概率模型产生。最常用的就是高斯混合聚类，模型的求解使用EM算法迭代。
>
>求解思想就是，初始化高斯混合模型的参数，每一轮迭代中求出每个样本在每个高斯混合成分生成的后验概率，通过最大化似然函数来更新模型参数，各高斯混合成分的均值可用过样本的加权（后验概率）平均估计，协方差矩阵也同样由样本的加权协方差矩阵估计，每个高斯混合成分的混合系数由该成分的平均后验概率确定。这里的后验概率就相当于每个样本在每个成分上的加权。
>
>优点：
>
>1. 对于类的划分以概率的形式呈现，不那么“坚硬”；
>
>缺点：
>
>1. 执行效率不高，特别是对于分布数量较多但数据量很小的时候。

##### 7.层次聚类有哪些算法，优缺点如何

>
>
>

