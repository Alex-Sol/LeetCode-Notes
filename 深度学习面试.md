[TOC]



##### 1.感受野的计算

>感受野用来表示网络内部不同位置的神经元**对原图像的感受范围的大小**。需要明白，神经元是无法对原始图像的所有信息进行感知的，因为这些网络结构中普遍使用了卷积层和pooling层，在层与层之间均为局部连接(sliding filter)。神经元感受野的值越大表示其能接触到的原始图像的范围就越大，也就意味着它更能蕴含更为全局、语义层次更高的特征；而值越小则表示其所包含的特征就越趋于局部和细节，**因此感受野的值大致可以用来判断每一层的抽象层次。**
>
>计算每一层感受野的思路是用总的感受野，减去重叠的部分。例如两层卷积网络，第一层k=3, stride=2, 第二层k =2, stride =1，那么感受野计算为 3x2 - (3-2)x(2-1)，这里的3-2是第一层的重叠部分，2-1表示第二个卷积层卷积核为2，收到第一层的重叠部分数目为1，如果第二层卷积核为3，那么使用3-1。高层时候，重叠部分是相乘的。总结一下，最后的计算公式为 
>
>$$ r_n=r_{n-1}\times k_n-(r_{n-1}-\prod\limits^{n-1}_{i=1} s_i)\times(k_n-1)​$$。
>
>可以化简为
>$$
>r_n=r_{n-1}+(k_n-1)\prod\limits_{i=1}^{n-1}s_i
>$$
>一个conv5x5的感受野相当于堆叠两个conv3x3，但是不同的是一个conv5x5的参数量要比两个conv3x3的参数量要大，这也是Inceptionv2一个重要的创新点，多层卷积堆叠的感受野和一层是一样的，但是参数量大大减少。
>
>例如:feature_map: 64x64x128->64x64x64，那么一个conv5x5的参数量为 5x5x128x64，而连续的两个conv3x3的参数量（中间层为深度为 64）3x3x128x64+3x3x64x64。
>
>另外，一些需要记住的要点：
>
>1. 步长为1的卷积层线性地增加感受野，深度网络可以通过堆叠多层卷积网络增加感受野；
>2. 步长为2的下采样层乘性地增加感受野，但是受限于输入分辨率不能随意增加；
>3. 步长为1的卷积层在网络后面会比在网络前面增加更多的感受野；
>4. 深度CNN的感受野往往是大于输入分辨率的，比如ResNet-101输出层的感受野比输入分辨率3.7倍；
>5. 深度CNN为保持分辨率每个conv都要加padding，所以等效到输入图像的padding非常大。

##### 2.卷积层输出feature_map大小的计算

>使用公式
>$$
>f_{out}=(f_{in}-r+2P)/S+1
>$$
>其中 r 为卷积核大小，P为padding大小，而S为步长stride。

##### 3.激活函数的选择

>激活函数在神经网络中增加了非线性，没有激活函数的神经网络本质上就是一个线性回归模型，引入非线性激活函数之后，神经网络才可以逼近任意的函数。另外，激活函数使反向传播成为可能，激活函数的误差梯度可以用来调整权值和偏差。
>
>常用的有Sigmoid, Tanh和 ReLU。
>
>Sigmoid：将正无穷到负无穷的值压缩为0到1的范围，主要有两个缺点
>
>缺点：
>
>1. 容易饱和，当输入非常大或者非常小时，神经元的梯度就接近于0，在反向传播中会导致权值无法更新。使用的时候，需要注意输入参数的初始值，避免饱和。
>2. 输出不是0均值，如果每一层神经元的输入都为正，那么对权值的局部梯度都为正，权值要么都向正方向更新，要么都向负方向更新，出现一种捆绑效果，收敛会非常缓慢。
>
>Tanh：和Sigmoid基本相同，只是tanh把输出压缩到了-1-1的范围，因此它是0均值的，但是还是存在梯度饱和的问题。
>
>ReLU：
>
>优点：
>
>1. 收敛速度很快，因为梯度不会饱和。
>2. 只需要计算一个阈值，不需要向Sigmoid和tanh那样计算指数，计算复杂度低。
>
>缺点：
>
>1. ReLU很容易导致神经元的失活，如果一个神经元在输出值小于0，那么它的负梯度会被置零，这个神经元可能不会再被任何的数据激活，这个神经元在网络中就失去了作用。实际操作的时候，如果学习率设的很大，那么很容易出现这个问题。
>
>具体对于ReLU的改进就是相办法在让负区间的梯度不为0，具体的有Leaky ReLU，在负区间设一个较小的斜率，效果最好的是Maxout。
>
>Maxout:
>
>传统的MLP（多层感知机）算法在两层之间参数只有一组，Maxout是在一组同时训练多组参数，然后选择激活值最大的作为下一层神经元的激活值，相当于在两层之间又多了一层，多的一层的神经元个数是超参数，这层神经元最大的输出值传到下一层。所以使用Maxout时参数会成倍的增加，另外，maxout相当于有一个跨通道池化的操作，如果maxout下一层是max pooling操作，那么可以将max pooling的空间池化和maxout的通道池化合并在一起，都放到maxout中。
>
>单个Maxout函数可以理解为一种分段线性函数来近似任意凸函数（任意的凸函数都可以由分段线性函数来拟合），maxout分段的个数是设置的，它在每一个分段处都是线性的。
>
>优点：
>
>1. maxout没有上下界，不会产生饱和的问题。

##### 4.dropout的要点和使用

>使用要点：
>
>dropout 在更新时使用更大的步长更有效，这样可以在不同的训练子集上对不同的模型有明显的影响来使目标函数有持续的波动性，理想情况下整个训练过程就类似于bagging来训练集成的模型（不过带有参数共享的约束）。一般的SGDG更新时会使用更小的步长，使目标函数朝着最可能的方向平稳的移动。
>
>另外，dropout会使relu在0值饱和的时间超过60%，很容易导致神经元的失活，而普通的SGD只会使relu的0饱和时间小于5%。而使用maxout函数不存在这个问题，梯度在maxout单元上总是能够传播，即使maxout出现了0值，这些0值的神经元依然可以再次被激活。只有比例较高且无法改变的神经元失活才会损害网络的性能。
>
>和bagging集成方法的异同：
>
>对于深度网络模型，dropout只能作为模型平均的一种近似。在dropout训练的集成模型中，所有的模型都只包含部分输入和部分隐层参数，对每一个训练样本，我们都会训练一个包括不同隐含层参数的子模型。dropout和bagging的**相同点**是不同的模型使用不同的数据子集，**不同点 **是dropout的每个模型都只训练一次并且所有的模型参数共享。

##### 5.Inception的模型发展及创新

>
>
>