[TOC]

*- - - 用于面试的整理*

Random Forest, GBDT和 XGBoost都属于集成学习，目的是通过结合多个基学习器的预测结果来改善单个学习器的泛化性能和鲁棒性。

### 1.分类

目前集成学习分为两大类：即个体学习器之间存在强依赖关系、必须串行生成的序列化方法，和 个体学习器间不存在强依赖关系、可同时生成的并行化方法。

Boosting是前者的代表，Bagging和随机森林是后者的代表。

Boosting 提升方法从基学习器出发，反复学习，得到一系列弱学习器，进行组合。Adaboost提高前一轮弱分类器分类错误的样本的权值，降低那些正确分类样本的权值。弱分类器组合时采用加权多数表决的方法。

##### 梯度提升

梯度提升利用损失函数的负梯度在当前模型的值作为回归问题提升树算法的残差近似值，用来拟合回归树，这是利用最速下降法的近似方法。

对于分类问题，由于样本输出的不是连续值，因而无法直接从输出类别中拟合类别输出的误差，对于此有两种解决办法：

1. 利用指数损失函数，此时GBDT退化为Adaboost算法；
2. 利用类似逻辑回归的<u>对数似然损失函数</u>，也就是用类别的预测概率值和真实概率值的差进行拟合。

##### GBDT的正则化

1. 和Adaboost类似，设置学习步长，较小的步长意味着需要更多的弱学习器的迭代次数，通常用步长和迭代最大次数一起决定算法的拟合效果；
2. 对样本进行采样，每次只用一部分样本去做GBDT决策树的拟合；
3. 对弱学习器的CART回归树进行剪枝；

##### GBDT优缺点

GBDT主要的优点有：

　1) 可以灵活处理各种类型的数据，包括连续值和离散值。

　2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。

　3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

GBDT的主要缺点有：

　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。

##### 提升树

采用加法模型与前向分步算法，以决策树为基函数的提升方法称为提升树。加法模型的基本思想就是每次学习一点，逐渐逼近最重要预测的值。这个优化问题十分复杂，通常使用前向分步算法来优化。针对不同的提升树学习算法，主要区别在于使用的损失函数不同，包括使用平方误差损失函数的回归问题，用指数损失函数的分类问题（即Adaboost算法）。

##### 为什么要使用梯度提升

当采用平方误差损失函数时$L(y,f(x))=(y-f(x))^2$，这个形式刚好可以转换为$L(\cdot)=[y-f_{m-1}(x)-T(x;\theta_m)]^2=[r-T(x;\theta_m)]$，意思就是第m步使用损失函数最小求解的树就相当于拟合数据的残差。这时复杂的优化损失函数的问题就很简单了，因为等价于拟合当前模型在数据上的残差。

但是如果换个损失函数，损失函数的优化就变得很棘手，这时我们利用损失函数的负梯度方向在当前模型的值作为回归提升树算法的残差近似值，拟合一个回归树。

##### XGBoost

该算法思想就是不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数，去拟合上次预测的残差。

基于空间切分去构造一颗决策树是一个NP难问题，我们不可能去遍历所有树结构，因此，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用上式目标函数值作为评价函数。

当样本的第i个特征值缺失时，无法利用该特征进行划分时，XGBoost的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。

##### 优点

1. 目标函数优化利用了损失函数关于待求函数的二阶导数

2. 使用许多策略去防止过拟合，如：正则化项、Shrinkage and Column Subsampling等。

3. 支持并行化，这是XGBoost的闪光点，虽然树与树之间是串行关系，但是同层级节点可并行。具体的对于某个节点，节点内选择最佳分裂点，候选分裂点计算增益用多线程并行。训练速度快。

#### 常考问题

##### 1.XGboost为什么要用泰勒展开，优势在哪里？

>xgboost使用了一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，只用泰勒展开取得<u>函数做自变量</u>的二阶导数形式，可以在不选定具体损失函数的情况下，<u>仅仅依靠输入数据的值就进行叶子分裂优化计算，本质上就把损失函数的选取和模型算法优化/参数选择分开了，</u>这种去耦合增加了xgboost的适用性，使得它按需选取损失函数，既可用于分类，也可用于回归。

##### 2.XGboost如何寻找最优特征，是有放回还是无放回？

>xgboost在训练过程中给出各个特征的增益评分，最大增益特征会被选出来作为分裂依据，从而记忆了每个特征对在模型训练时的重要性---从根到叶子中间结点涉及某特征的次数作为该特征重要性排序。
>
>xgboost属于boosting集成学习方法，样本是不放回的，因而每轮计算样本不重复。另一方面，xgboost支持子采样，也就是每轮计算可以不适用全部样本，以减小过拟合。进一步，xgboost还有列采样，每轮计算按百分比随机采样一部分特征，既提高运算速度又减小过拟合。
>

##### 3.随机森林的优势

>优势：
>
>1. 模型之间的训练可以高度并行化，速度快；
>2. 可以随机选择决策树结点划分特征，这样在样本特征维度很高的时候，仍可以高效训练模型；
>3. 训练以后，可以给出各个特征对于输出的重要性；
>4. 由于采用了随机采样，训练出的模型方差较小，泛化能力强；
>5. 相对于Boosting系列的Adboost和GBDT，RF的实现比较简单；
>6. 对部分特征的缺失不敏感。
>
>缺点：
>
>1. 在噪音比较大的样本集上，RF容易陷入过拟合；
>2. 取值划分比较多的特征容易对RF的决策产生更大的影响。
>

##### 4.简要描述boosting方法的思想。

>在不改变数据集的情况下，通过在迭代训练弱分类器中，不断提高被错误分类样本的权重（在决策树中可以理解为样本的分布），不断减少正确分类样本的权重，最后通过加权线性组合弱分类器得到最终的分类器。正确率越高的弱分类器的投票权数越大，反之越低。

##### 5.简要的介绍GBDT。

>GBDT的基本原理是boosting里面的boosting tree，因为GBDT需要按照损失函数的梯度近似拟合残差，这样拟合的是连续的值，因此只有回归树。与传统的boosting相比，GBDT每一次弱分类器的学习都是为了减少上一次模型的残差，为了消除残差，可以在残差减少的梯度方向上建立一个新的模型。所以，在GBDT中，每个新的模型的建立是为了使之前的模型的残差往梯度方向减少，这与传统的Boosting改变正确、错误样本的权重有着很大的区别。
>
>当然，为了实现用GBDT来分类，我们可以使用类似于逻辑回归的对数似然损失函数的方法，用类别的预测概率值和真实概率值的残差来拟合损失。
>
>Boosting tree适合于平方损失和指数损失，而GBDT适合于各类损失函数。当损失函数为平方损失，则相当于boosting tree拟合残差；损失函数为指数损失，则近似于adaboost，但树是回归树。

##### 6.GBDT常用的损失函数

>1. 指数损失函数，$L(y,f(x))=\exp(-yf(x))$，具体的负梯度计算和adaboost相似；
>2. 对数损失函数，用于分类算法；
>3. 均方差损失：相当于直接拟合残差；
>4. 绝对值损失：
>5. Huber损失：均方差损失和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近的点采用均方差，这个界限一般用分位数点度量。
>6. 分位数损失：对应分位数回归的损失函数，和Huber主要用于健壮回归，减小异常点对损失函数的影响。

##### 7.GBDT的正则化

>1. 最常用的正则化项就是加上一个学习步长，较小的学习步长意味着需要更多的弱学习器。
>2. 通过子采样比例，这里是不放回的抽样，可以防止过拟合，减小方差，但是会增加偏差。
>3. 对树进行正则化剪枝。

##### 8.XGBoost与GBDT的不同

>1. 损失函数使用二阶导数进行逼近；
>2. 对树的结构进行了正则化约束，防止模型复杂度过高；
>3. 节点分裂方式不同，GBDT使用的是基尼系数，XGBoost使用的是由目标函数推导出的分裂标准。

##### 9.XGBootst其它的优化

>1. 在寻找最优分割点时，考虑传统的枚举每个特征所有的分割点的贪心效率太低，xgboost根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据求分割点的公式找出最优分割点。
>2. xgboost借鉴了随机森林中的列采样技术，即在某个节点分类时，从当前属性集合中的某些属性中选择最优分裂属性，这种方法降低了过拟合。
>3. 考虑了训练数据为稀疏值的情况，可以为缺失值或者为属性指定默认的分支方向，大大提高了算法的效率。

##### 10.XGBoost为什么使用深度很小的树就能达到很好的效果

>Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。

##### 11.XGBoost和GBDT如何处理多分类问题

>首先，GBDT已经不是使用基尼系数来进行特征点分割了，基尼系数只适用于分类问题，而GBDT和xgboost本质上是个回归问题，使用GBDT来解决多分类问题，实际上是把它转换为回归问题。
>
>再多分类问题中，假设有 k 个类别，那么每一轮实质上是构建了 k 颗树，对某个样本的预测值为 $f_1(x),f_2(x),...,f_k(x)$，然后使用softmax 来产生分类的概率值，属于某个类别 c 的概率为 
>$$
>p_c=\frac{\exp(f_c(x))}{\sum_k \exp(f_k(x))}
>$$
>此时该样本的 loss 可以用交叉熵损失表示，并且对每个 $f_i(x)$ 都可以算出一个梯度，$f_1,,, f_k$   可以算出当前轮的残差，供下一轮迭代学习。
>
>最终预测的时候，输入的 x 会得到 k 个输出值，然后通过 softmax 获得属于各类别的概率值。

